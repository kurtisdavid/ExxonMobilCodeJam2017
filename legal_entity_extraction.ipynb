{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExxonMobil CodeJam July 2017\n",
    "\n",
    "This notebook follows the work our team went through to extract legal entities from contracts. It will have the following sections:\n",
    "\n",
    "### 1. Load Text Files\n",
    "### 2. NER Tagging (Initial)\n",
    "### 3. Preparing New Training Set\n",
    "### 4. Testing Results\n",
    "\n",
    "The reason for this order was because we initially believed Stanford's NER Tagger would be able to recognize all of our supplier companies, however this was not the case. Therefore we had to spend the latter half of the hackathon creating our own corpus to retrain their model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "import string\n",
    "import pickle\n",
    "import chardet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the contracts saved as text files\n",
    "txt_directory = '<insert directory here>/*.txt' #can't place the actual directory used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_files = glob.glob(txt_directory) #returns a list of every text file in the given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe based on a list of text files (Column 1: Filename; Column 2: Text)\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "-----------\n",
    "txt_files - list of strings that contain the directories for each text file\n",
    "\n",
    "Returns:\n",
    "--------\n",
    "DataFrame - pandas DataFrame with 2 columns (filename, text in file)\n",
    "'''\n",
    "def load_txt(txt_files):\n",
    "    \n",
    "    text_dict = {'File': [], 'Text': []}\n",
    "    for filename in txt_files:\n",
    "        \n",
    "        with open(filename, 'r', encoding = 'utf-8') as file:\n",
    "            \n",
    "            text = file.read().strip()\n",
    "        \n",
    "        name = filename.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        text_dict['File'].append(name)\n",
    "        text_dict['Text'].append(text)\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(text_dict, columns = ['File', 'Text'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_txt(text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_directory = '<insert where stanford-ner 3 class classifier was saved>'\n",
    "ner_jar_directory = '<insert stanford-ner jar directory>'\n",
    "\n",
    "st = StanfordNERTagger(model_directory,\n",
    "                       ner_jar_directory, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extractLegalEntities is the main function to collect the information we wanted (Supplier + ExxonMobil Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "-----------\n",
    "df - dataframe containing two columns: (filename, text in file)\n",
    "tagger - instance of Stanford NER Tagger through nltk's API\n",
    "\n",
    "Returns:\n",
    "--------\n",
    "DataFrame - pandas DataFrame containing desired entities found in each contract file\n",
    "\n",
    "'''\n",
    "def extractLegalEntities(df, tagger):\n",
    "    \n",
    "    # initialize dictionary\n",
    "    entities = {'File': [], 'Supplier': [], 'ExxonMobil Entity': []}\n",
    "    \n",
    "    # go through each file in df (text dataframe)\n",
    "    for row in df.itertuples():\n",
    "        \n",
    "        filename = row[1]\n",
    "        text = row[2]\n",
    "        \n",
    "        # use the inputted tagger to tokenize current text\n",
    "        tags = tagger.tag(word_tokenize(text))\n",
    "        \n",
    "        # collect the first 2 continuous chunks of words labeled as organizations\n",
    "        # source: https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\n",
    "        orgs = []\n",
    "        count = 0\n",
    "        for tag, chunk in groupby(tags, lambda x:x[1]):\n",
    "            if count == 2:\n",
    "                break\n",
    "            if tag == 'ORGANIZATION':\n",
    "                count += 1\n",
    "                orgs.append(' '.join(w for w, t in chunk))\n",
    "        \n",
    "        \n",
    "        entities['File'].append(filename)\n",
    "        \n",
    "        # determine which org found was the ExxonMobil entity, the other must be supplier\n",
    "        found = False\n",
    "        for org in orgs:\n",
    "            \n",
    "            if not found and ('ExxonMobil' in org or 'Exxon' in org or 'Mobil' in org):\n",
    "                \n",
    "                entities['ExxonMobil Entity'].append(org)\n",
    "                found = True\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                entities['Supplier'].append(org)\n",
    "                \n",
    "        \n",
    "    return pd.DataFrame(entities, columns = ['File', 'ExxonMobil Entity', 'Supplier'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Results:\n",
    "\n",
    "Because each contract has private information, I'm unable to give an example of the results. However, the main key points we noted were:\n",
    "\n",
    "- Accuracy of obtaining correct entities was 40% (out of 15 files)\n",
    "- The original tagger could not recognize some suppliers as organizations (a few were identified as locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Thus because of these results, we opted to go another route to try and improve the accuracy: building a new corpus and retrain their model on it: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing new Training Set\n",
    "\n",
    "A lot of this section is ad-hoc and multiple developers scraped data so most of it will work on pulling everything together into one training text file.\n",
    "\n",
    "** Due to the constraint on time (only had 10 hours) we did not look deep into the needed format for training. Following Stanford NER's FAQ, we saw it should be in this format:**\n",
    "\n",
    "word_1 \\t label \n",
    "\n",
    "word_2 \\t label\n",
    "\n",
    "...\n",
    "\n",
    "word_n \\t label\n",
    "\n",
    "** Here is a link to the FAQ + Tutorial on how to train your own: https://nlp.stanford.edu/software/crf-faq.shtml#a**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because another member of the team scraped all ExxonMobil entities and pickled it, I had to load it into this notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled dictionary containing ExxonMobil entities\n",
    "companies_pickled = '<insert directory here>'\n",
    "\n",
    "with open(companies_pickled, 'rb') as pickle_file:\n",
    "    \n",
    "    companies = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The vendors were also collected by another member, so I collected the information in the given text file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "vendors_txt_directory = '<insert vendor directory here>'\n",
    "with open(vendors_txt_directory,'r') as f:\n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        if c > 0:\n",
    "            \n",
    "            lines.append(line.strip())\n",
    "        \n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine companies + vendors into one list\n",
    "names = companies.values()\n",
    "names = list(names) + lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530599"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe that splits each entity into space separated tokens (putting each in a unique column)\n",
    "\n",
    "df_companies = pd.DataFrame(list(names), columns = ['Name'])\n",
    "\n",
    "splitted = pd.DataFrame([x.split() for x in df_companies['Name'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534479"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because each entity was still located in a single row (separated into multiple columns per token), I could then go through and filter out punctuation as well as None values. The None values indicated that there are no more tokens for that entity.**\n",
    "\n",
    "**We opted to remove punctuation from the corpus, but that may actually help with future work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n"
     ]
    }
   ],
   "source": [
    "parts = []\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "count = 0\n",
    "for row in splitted.iterrows():\n",
    "    \n",
    "    count+=1\n",
    "    \n",
    "    if (count%10000 == 0):\n",
    "        \n",
    "        print(count)\n",
    "    \n",
    "    for part in row[1]:\n",
    "        \n",
    "        if not part is None:\n",
    "            \n",
    "            part = part.translate(translator)\n",
    "            \n",
    "            if part == '':\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            parts.append(part)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forgot to add the label so added here\n",
    "for i in range(len(parts)):\n",
    "    \n",
    "    parts[i] += '\\tORG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "** Saving current results: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('parts.pickle', 'wb') as file:\n",
    "    \n",
    "    pickle.dump(parts,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** While we were training on this dataset, we ended up with an error saying we had too many words (unlikely). This could have also been because we ONLY had ORG words (positive class) and not regular/filler words (the negative class). So initially I removed all repeats: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = list(set(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = '\\n'.join(parts)\n",
    "\n",
    "# write the prepared training data to be used in the model\n",
    "with open('trainingData.txt', 'w') as f:\n",
    "    \n",
    "    f.write(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results were very skewed using this first trial of training, so our team went and extracted the filler words in a contract itself. Note because this was from a single contract, then not that many were added... in the future if this can be done on more documents, then the results should definitely improve. (Did not have time to review more the day of CodeJam).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "contract_filler_words_directory = '<insert contract words directory here> (.txt)'\n",
    "with open(contract_filler_words, 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        lines.append(line.strip())\n",
    "        \n",
    "big_doc = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(set(word_tokenize(big_doc)))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add in 'O' label\n",
    "parts2 = parts[:]\n",
    "for token in tokens:\n",
    "    \n",
    "    parts2.append(token + '\\tO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = '\\n'.join(parts2)\n",
    "\n",
    "# write new training data\n",
    "with open('trainingData2.txt', 'w') as f:\n",
    "    \n",
    "    f.write(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save current results\n",
    "with open('parts2.pickle', 'wb') as file:\n",
    "    \n",
    "    pickle.dump(parts2,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Because 900 extra words would not be enough to balance the datasets, we quickly found a repository containing the top 10000 used words in english inside a repo here: https://github.com/first20hours/google-10000-english**\n",
    "\n",
    "** Unfortunately the classes are still unbalanced, but it was better than before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('google-10000-english-no-swears.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        lines.append(line.strip())\n",
    "        \n",
    "big_doc = ' '.join(lines)\n",
    "\n",
    "# Same steps as before...\n",
    "\n",
    "tokens = list(set(word_tokenize(big_doc)))\n",
    "len(tokens)\n",
    "\n",
    "parts3 = parts2[:]\n",
    "for token in tokens:\n",
    "    \n",
    "    parts3.append(token + '\\tO')\n",
    "\n",
    "total = '\\n'.join(parts2)\n",
    "\n",
    "with open('trainingData3.txt', 'w') as f:\n",
    "    \n",
    "    f.write(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training each new dataset, we ran this simple test to see how it was improving in generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'ORG'),\n",
       " ('my', 'O'),\n",
       " ('name', 'O'),\n",
       " ('is', 'O'),\n",
       " ('ExxonMobil', 'ORG'),\n",
       " ('Global', 'ORG'),\n",
       " ('Services', 'ORG'),\n",
       " ('Company', 'ORG')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_directory = '<insert new model directory here (after training)>'\n",
    "\n",
    "\n",
    "st = StanfordNERTagger(model_directory,\n",
    "                       ner_jar_directory, encoding='utf-8')\n",
    "st.tag('Hi my name is ExxonMobil Global Services Company'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above were the results of our final training set; 'Hi' was still being labeled as an ORG but significantly better than our first attempt (where everything got labeled as an ORG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In terms of the goal at hand though, the corpus was far from being good enough to extract the legal entities accurately (worse than Stanford's NER tagger). Because we had only gone through preliminary iterations of the training set, this can certainly be improved with the following:**\n",
    "\n",
    "- Improved data quality of supplier/EM tokens\n",
    "- More understanding of how to set up the training set. This is what I'd first look into, because from Stanford's tutorial, repeats definitely exist. In addition, can the sequence of words in the file affect the model? In their tutorial, they utilized a chapter from a book as a training set (which is sequenced). Our way of creating the training data was not only skewed, but the classes were never intermixed (where the large ORG block came first in the file and the small O block followed). This may be fixed by using Stanford's provided tokenizer. \n",
    "- Increasing our contract filler ('O') words in the corpus. This can be done by manually reviewing contract files and removing ORG information and streaming the words into the corpus as done in Step 3.\n",
    "\n",
    "Overall, improvement will be an iterative process, but certainly the work we've done is promising for the problem at hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
